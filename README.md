# RAG
different methods of use RAG

This Repository take the pdf file as input and able to store it in chroma DB and able to answer the questions related to the uploaded file



Below is the different types of RAG Evaluation types

1. Retrieval Evaluation
    This measures how effectively the retriever fetches relevant documents or passages. Common metrics include:

    Recall@K: The proportion of relevant documents among the top-K retrieved.
    Precision@K: The proportion of retrieved documents that are relevant in the top-K.
    Mean Reciprocal Rank (MRR): Evaluates the rank position of the first relevant document.
    Normalized Discounted Cumulative Gain (nDCG): Focuses on the relevance of documents retrieved at various ranks.
    Coverage: Measures if all necessary information for the task is retrieved.
2. Generation Evaluation
    This measures the quality of the outputs generated by the language model.

    Fluency: Assesses grammatical correctness and coherence.
    Relevance: Evaluates how well the generation aligns with the query and the retrieved context.
    Factual Accuracy: Ensures the generated response is factually consistent with the retrieved documents.
    Hallucination Detection: Measures how often the model generates unsupported or incorrect facts.
3. End-to-End Task-Specific Evaluation
    This evaluates the RAG pipeline holistically based on the final task's success. Metrics vary depending on the application, such as:

    QA Systems:
    Exact Match (EM): Measures if the output matches the ground truth.
    F1 Score: Measures overlap between the generated answer and the ground truth.
    Summarization:
    ROUGE: Measures overlap of n-grams between the summary and a reference.
    BLEU: Measures the precision of generated n-grams.
    Conversational Agents:
    Dialog Quality Assessment: Includes appropriateness, informativeness, and engagement.
4. Human Evaluation
    Humans assess the quality of the system based on subjective measures.

    Relevance and Factuality: Rating the response's correctness.
    User Satisfaction: Measuring user-perceived helpfulness or accuracy.
    Error Analysis: Identifying specific weaknesses in retrieval or generation.
5. Latency and Efficiency Metrics
    Performance metrics focusing on operational efficiency:

    Latency: Measures how fast the system responds to a query.
    Throughput: Assesses how many queries the system can process in a given timeframe.
    Scalability: Evaluates performance under increasing workloads.
6. Ablation Studies
    Ablation involves modifying or removing specific components to evaluate their impact on performance:

    Retriever vs. Generator Impact: Isolating and testing the retriever and generator independently.
    Context Length Effects: Testing performance with varying amounts of retrieved context.
7. Robustness Evaluation
    Testing the systemâ€™s resilience to edge cases and noisy inputs:

    Adversarial Examples: Introducing deliberately misleading queries or documents.
    Noisy Data Handling: Assessing performance with incomplete, redundant, or irrelevant information.
8. Explainability Evaluation
    Evaluating how well the RAG system explains its responses:

    Rationale Extraction: Testing whether the model provides understandable justifications based on retrieved documents.
    Traceability: Ensuring clear mapping from input to output.
9. User-Centric Evaluation
    Focusing on how well the system meets end-user needs:

    Interaction Usability: Measuring ease of use in iterative or conversational queries.
    Personalization Metrics: Evaluating performance for personalized recommendations or responses.



##################### In My Code #################

in the following code I have used Ollama server using docker
The commandas are below



Apart from that, The Evaluation metrics I have used is

1. Recall@K: The proportion of relevant documents among the top-K retrieved.
2. Precision@K: The proportion of retrieved documents that are relevant in the top-K.
3. Similarity: Finding the cosine similarity between retrieval answer with respect to the ground truth

4. In order to handle hallucination I have tried different approaches of chunking the details e.g.
 fixed length with overlapping, topic based, semantic based as well as heading based
